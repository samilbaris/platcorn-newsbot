import os, re, time, sqlite3, sys
from datetime import datetime
from urllib.parse import urlparse

import feedparser, requests
from deep_translator import GoogleTranslator

# Ã–zetleme & NLTK
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
import nltk
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

# Makale gÃ¶vdesi
from newspaper import Article

# =======================
# AYARLAR
# =======================
TELEGRAM_BOT_TOKEN = "7766727211:AAHl8BnCzQey74LKZb4mw6bFqqR5jZYqw-U"
TELEGRAM_CHAT_ID   = "8513010757"

INTERVAL_SECONDS     = 300      # 5 dkâ€™da bir kontrol
MAX_ITEMS_PER_FEED   = 5
SUMMARY_SENTENCES    = 4
TRANSLATE_TITLES     = True
TRANSLATE_SUMMARIES  = True

# ðŸŒ TÃ¼m anahtar kelimeler (TR + EN)
GLOBAL_KEYWORDS = [
    # ðŸ§© Platformlar & Ekosistem
    "youtube","twitch","kick","tiktok","instagram","x.com","threads","rumble",
    "livestream","stream","streamer","creator","influencer","content creator",
    "broadcast","subscriber","followers","viewers","shorts","clip","ban","partner",
    "community","platform","streaming","upload","algorithm","monetization","feature",
    "viral","trend","controversy","backlash","criticism","tepki","tepki Ã§ekti","linÃ§",
    "drama","reaksiyon","yayÄ±n yasaÄŸÄ±","trend oldu","viral oldu",

    # ðŸŒ ÃœnlÃ¼ yayÄ±ncÄ±lar & internet figÃ¼rleri
    "mrbeast","ishowspeed","hasanabi","asmongold","xqc","kai cenat",
    "ludwig","ninja","pokimane","amouranth","valkyrae","shroud","drdisrespect",
    "ice poseidon","adin ross","nickmercs","summit1g","tfue","sykkuno",
    "myth","pewdiepie","dream","tommyinnit","markiplier","jacksepticeye",
    "logan paul","ksi","jake paul","moistcr1tikal","charli dâ€™amelio","bella poarch",

    # ðŸ’¬ Trendler & topluluk dinamikleri
    "reaction","drama","controversy","leak","clip","highlight","rage quit",
    "cancelled","apology","comeback","announcement","collab","partnership",
    "reveal","exclusive","interview","livestream fail","viral clip","top moment",
    "trending","memes","internet reaction","eleÅŸtirildi","skandal","tartÄ±ÅŸma",
    "gÃ¼ndem oldu","sosyal medya tepki","yayÄ±ncÄ± kavgasÄ±","clash","fued","debate",

    # ðŸ’° Creator economy & dijital iÅŸ dÃ¼nyasÄ±
    "sponsorship","deal","brand","agency","marketing","ads","revenue",
    "creator economy","influencer marketing","merch","startup","partnership",
    "brand deal","promotion","sponsorluk","iÅŸbirliÄŸi","ajans","kampanya","kazan",
    "income","platform change","exclusive deal","collaboration","network",
    "marka anlaÅŸmasÄ±","kampanya","tanÄ±tÄ±m videosu","sponsorlu iÃ§erik",

    # ðŸ‡¹ðŸ‡· TÃ¼rkÃ§e karÅŸÄ±lÄ±klar & yerel dijital kÃ¼ltÃ¼r
    "yayÄ±ncÄ±","influencer","iÃ§erik Ã¼retici","dijital kÃ¼ltÃ¼r","sosyal medya",
    "viral","akÄ±m","banlandÄ±","yasaklandÄ±","iÅŸbirliÄŸi","sponsor","ajans","anlaÅŸma",
    "abonelik","yayÄ±n kazancÄ±","platform deÄŸiÅŸikliÄŸi","trend oldu","komik video",
    "reaksiyon","twitch dramasÄ±","kick yayÄ±nÄ±","youtube videosu","sÄ±zdÄ±rÄ±ldÄ±",
    "takipÃ§i","izlenme","tÄ±klanma","algoritma","viral oldu","yayÄ±n yasaÄŸÄ±",
    "tepki Ã§ekti","tepki gÃ¶rdÃ¼","eleÅŸtirildi","gÃ¼ndem oldu","linÃ§ yedi"
]

# ðŸ”¤ Ã‡eviri sÄ±rasÄ±nda dokunulmamasÄ± gereken Ã¶zel isimler
PROPER_NOUNS = [
    "YouTube", "Twitch", "Kick", "Rumble",
    "MrBeast", "iShowSpeed", "HasanAbi", "Asmongold", "xQc",
    "Kai Cenat", "Ludwig", "Ninja", "Pokimane", "Amouranth",
    "Valkyrae", "Shroud", "Dr Disrespect", "Platcorn"
]

# ðŸŒ Kaynak isim eÅŸlemesi (gÃ¶rÃ¼nÃ¼r isimler)
PUBLISHER_MAP = {
    "www.dexerto.com": "Dexerto",
    "www.theverge.com": "The Verge",
    "www.ign.com": "IGN",
    "www.vulture.com": "Vulture",
    "www.hollywoodreporter.com": "Hollywood Reporter",
    "www.variety.com": "Variety",
    "www.gamespot.com": "GameSpot",
    "www.pcgamer.com": "PC Gamer",
    "www.kotaku.com": "Kotaku",
    "www.gamerbraves.com": "Gamer Braves",
    "www.hypebeast.com": "Hypebeast",
    "www.onedio.com": "Onedio",
    "www.sportskeeda.com": "Sportskeeda",
    "www.complex.com": "Complex",
    "www.gamingbible.com": "GamingBible",
    "www.reddit.com": "Reddit / LivestreamFail",
}


# ðŸ—žï¸ Tek kategori: Platcorn & Creator dÃ¼nyasÄ±
CATEGORIES = {
    "ðŸŸ¢ Platcorn & Creator": {
        "feeds": [
            # â€” Ä°ngilizce yayÄ±ncÄ±/creator haberleri â€”
            "https://www.dexerto.com/feed",
            "https://www.dexerto.com/streaming/feed",
            "https://www.dexerto.com/entertainment/feed",
            "https://www.dexerto.com/esports/feed",
            "https://www.theverge.com/creator-economy/rss/index.xml",
            "https://www.ign.com/rss",
            "https://www.kotaku.com/rss",
            "https://www.pcgamer.com/rss",
            "https://www.gamespot.com/feeds/news",
            "https://www.gamerbraves.com/feed/",
            "https://variety.com/feed/",
            "https://www.hollywoodreporter.com/feed/",
            "https://www.vulture.com/rss/all.xml",
            "https://screenrant.com/feed/",
            "https://www.hypebeast.com/feed",
            "https://www.tubefilter.com/feed/",
            "https://www.socialmediatoday.com/rss",
            "https://creatorhook.com/feed/",
            "https://passionfroot.me/blog/rss.xml",

            # â€” Reddit (sadece haber tarzÄ± iÃ§erik iÃ§in) â€”
            "https://www.reddit.com/r/LivestreamFail/new/.rss",

            # â€” TÃ¼rkÃ§e teknoloji ve eÄŸlence kaynaklarÄ± â€”
            "https://onedio.com/rss",
            "https://www.webtekno.com/rss",
            "https://shiftdelete.net/feed",
            "https://www.technopat.net/feed/",
            "https://www.log.com.tr/feed/",
            "https://www.donanimhaber.com/rss/tum/",

            # â€” RSS.app Ã¼zerinden eklenen Ã¶zel kaynaklar â€”
            "https://rss.app/feeds/we2ENM1QjscyHS6V.xml",  # Sportskeeda (streamer news)
            "https://rss.app/feeds/we2ENM1QjscyHS6V.xml"   # Complex (streamer tag)
        ],
        "keywords": GLOBAL_KEYWORDS
    }
}

APP_DIR = os.path.join(os.path.expanduser("~"), ".newsbot")
DB_PATH = os.path.join(APP_DIR, "seen.db")

# =======================
# ARAÃ‡LAR
# =======================
def log(msg: str):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}", flush=True)

def ensure_app_dir():
    os.makedirs(APP_DIR, exist_ok=True)

def init_db():
    ensure_app_dir()
    conn = sqlite3.connect(DB_PATH)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS seen (
            id TEXT PRIMARY KEY,
            title TEXT,
            link TEXT,
            category TEXT,
            ts   INTEGER
        )
    """)
    conn.commit()
    return conn

def host_of(url):
    try:
        return urlparse(url).netloc
    except:
        return ""

def publisher_of(url):
    host = host_of(url).lower()
    return PUBLISHER_MAP.get(host, host)

def pretranslate_en(s: str) -> str:
    # EN kÄ±saltmalarÄ± geniÅŸlet (Ã§eviri daha doÄŸal olsun)
    if not s: return s
    s = re.sub(r"\$?(\d+(\.\d+)?)\s?B\b", r"\1 billion", s, flags=re.IGNORECASE)
    s = re.sub(r"\$?(\d+(\.\d+)?)\s?M\b", r"\1 million", s, flags=re.IGNORECASE)
    s = re.sub(r"\$?(\d+(\.\d+)?)\s?K\b", r"\1 thousand", s, flags=re.IGNORECASE)
    s = s.replace("â€™","'").replace("â€œ","\"").replace("â€","\"")
    return s

def postprocess_money_tr(s: str) -> str:
    # million/billion â†’ milyon/milyar; thousand â†’ bin
    s = re.sub(r"\b([0-9]+(?:[.,][0-9]+)?)\s*million\b", r"\1 milyon", s, flags=re.IGNORECASE)
    s = re.sub(r"\b([0-9]+(?:[.,][0-9]+)?)\s*billion\b", r"\1 milyar", s, flags=re.IGNORECASE)
    s = re.sub(r"\b([0-9]+(?:[.,][0-9]+)?)\s*thousand\b", r"\1 bin", s, flags=re.IGNORECASE)
    # $ iÅŸaretinin yeri
    s = re.sub(r"\$\s*([0-9])", r"$\1", s)
    s = s.replace(" $", " $")  # bÄ±rak
    return s

TITLE_VERB_MAP = [
    (r"\bleaks?\b", "ifÅŸa etti"),
    (r"\bclaims?\b", "iddia etti"),
    (r"\breveals?\b", "aÃ§Ä±kladÄ±"),
    (r"\bditching\b", "bÄ±rakmak"),
    (r"\bquits?\b", "bÄ±raktÄ±"),
    (r"\bwould make\b", "kazanacaÄŸÄ±nÄ±"),
    (r"\bslammed\b", "tepki Ã§ekti"),
    (r"\bshuts? down\b", "kapatÄ±ldÄ±"),
]

def polish_title_tr(tr: str) -> str:
    if not tr: return tr
    t = tr

    # tipik kÃ¶tÃ¼ Ã§evirileri dÃ¼zelt / fiil eÅŸleÅŸtirme
    for pat, rep in TITLE_VERB_MAP:
        t = re.sub(pat, rep, t, flags=re.IGNORECASE)

    # baÅŸÄ±/bitiÅŸi temizle, fazla boÅŸluklarÄ± at
    t = re.sub(r"\s+", " ", t).strip()

    # cÃ¼mle baÅŸÄ± bÃ¼yÃ¼k, kalan normal; Ã¶zel isimler korunacak
    if t and not t.endswith(('.', '!', '?')):
        t = t[0].upper() + t[1:]

    return t

def translate_en_to_tr(text: str, is_title=False) -> str:
    if not text: return text

    # Ã–zel isimleri yer tutucuya al
    placeholders = {}
    safe = text
    for i, name in enumerate(sorted(PROPER_NOUNS, key=len, reverse=True)):
        key = f"__PN{i}__"
        placeholders[key] = name
        safe = re.sub(rf"\b{name}\b", key, safe, flags=re.IGNORECASE)

    safe = pretranslate_en(safe)

    try:
        tr = GoogleTranslator(source="en", target="tr").translate(safe)
    except Exception:
        tr = safe

    # Yer tutucularÄ± geri koy
    for key, name in placeholders.items():
        tr = tr.replace(key, name)

    tr = postprocess_money_tr(tr)

    if is_title:
        tr = polish_title_tr(tr)

    return tr

def fetch_article_text(url: str) -> str:
    try:
        art = Article(url)
        art.download(); art.parse()
        return (art.text or "").strip()
    except Exception:
        return ""

def summarize_en(text: str, n_sent: int) -> str:
    text = re.sub(r"\s+", " ", (text or "").strip())
    if len(text.split()) < 60:
        return text
    try:
        parser = PlaintextParser.from_string(text, Tokenizer("english"))
        summarizer = LsaSummarizer()
        sents = summarizer(parser.document, n_sent)
        return " ".join(str(s) for s in sents)
    except Exception:
        return text

def bullets_tr(paragraph: str) -> str:
    """Uzun paragrafÄ± 3-5 maddeye bÃ¶lmeye Ã§alÄ±ÅŸ (TR Ã§Ä±ktÄ±yÄ± daha okunur yapar)."""
    if not paragraph: return paragraph
    sents = re.split(r"(?<=[.!?])\s+", paragraph)
    sents = [s.strip() for s in sents if s.strip()]
    if len(sents) <= 1:
        return paragraph
    sents = sents[:5]
    return "â€¢ " + "\nâ€¢ ".join(sents)

def tg_send(text: str):
    try:
        url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
        requests.post(url, json={
            "chat_id": TELEGRAM_CHAT_ID,
            "text": text,
            "disable_web_page_preview": False
        }, timeout=30).raise_for_status()
    except Exception as e:
        log(f"Telegram gÃ¶nderim hatasÄ±: {e}")

def norm_id(entry) -> str:
    return getattr(entry, "id", None) or getattr(entry, "link", None) or getattr(entry, "title", "")

def already_seen(conn, _id: str) -> bool:
    cur = conn.execute("SELECT 1 FROM seen WHERE id=?", (_id,))
    return cur.fetchone() is not None

def mark_seen(conn, _id: str, title: str, link: str, category: str):
    conn.execute(
        "INSERT OR IGNORE INTO seen (id,title,link,category,ts) VALUES (?,?,?,?, strftime('%s','now'))",
        (_id, title, link, category)
    )
    conn.commit()

def match_keywords(title: str, kw_list) -> bool:
    if not kw_list:
        return True
    t = (title or "").lower()
    return any(k.lower() in t for k in kw_list)

def build_feed_catalog():
    catalog = {}
    for cat, spec in CATEGORIES.items():
        for f in spec.get("feeds", []):
            catalog[f] = cat
    return catalog

def run_once():
    conn = init_db()
    catalog = build_feed_catalog()
    sent_total = 0

    for feed_url, category in catalog.items():
        try:
            d = feedparser.parse(feed_url)
            entries = d.entries[:MAX_ITEMS_PER_FEED]
        except Exception as e:
            log(f"Feed hatasÄ±: {feed_url} -> {e}")
            continue

        cat_keywords = CATEGORIES.get(category, {}).get("keywords", [])

        for e in entries:
            _id   = norm_id(e)
            title = getattr(e, "title", "(baÅŸlÄ±ksÄ±z)")
            link  = getattr(e, "link", "")

            if already_seen(conn, _id):
                continue

            if not (match_keywords(title, GLOBAL_KEYWORDS) or match_keywords(title, cat_keywords)):
                mark_seen(conn, _id, title, link, category)
                continue

            base_text   = fetch_article_text(link) or getattr(e, "summary", "") or getattr(e, "description", "")
            summary_en  = summarize_en(base_text, SUMMARY_SENTENCES)

            title_out   = translate_en_to_tr(title, is_title=True) if TRANSLATE_TITLES else title
            text_tr     = translate_en_to_tr(summary_en, is_title=False) if TRANSLATE_SUMMARIES else summary_en
            text_final  = bullets_tr(text_tr)

            pub = publisher_of(link)
            msg = f"{category}\n<b>{title_out}</b>\nKaynak: {pub} ({host_of(link)})\n\n{text_final}\n\nðŸ”— {link}"

            try:
                tg_send(msg)
                mark_seen(conn, _id, title, link, category)
                sent_total += 1
                time.sleep(0.8)
            except Exception as ex:
                log(f"GÃ¶nderim hatasÄ±: {title} -> {ex}")

    log(f"GÃ¶nderilen yeni Ã¶zet: {sent_total}")

def main():
    if INTERVAL_SECONDS <= 0:
        run_once()
        return
    log(f"BaÅŸladÄ±. Her {INTERVAL_SECONDS} snâ€™de bir kontrol edilecek.")
    while True:
        run_once()
        time.sleep(INTERVAL_SECONDS)

if __name__ == "__main__":
    main()
