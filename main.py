# -*- coding: utf-8 -*-
import os, re, time, sqlite3, sys, hashlib
from datetime import datetime
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

import feedparser, requests
from deep_translator import GoogleTranslator

# Ã–zetleme & NLTK
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
import nltk
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

# Makale gÃ¶vdesi
from newspaper import Article

# =======================
# AYARLAR
# =======================
# Ortam deÄŸiÅŸkeni > sabit: GitHub Actions / PythonAnywhere ile gÃ¼venli
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN", "7766727211:AAHl8BnCzQey74LKZb4mw6bFqqR5jZYqw-U")
TELEGRAM_CHAT_ID   = os.getenv("TELEGRAM_CHAT_ID",  "8513010757")

INTERVAL_SECONDS     = 300      # 5 dkâ€™da bir kontrol (lokalde/PAâ€™de dÃ¶ngÃ¼)
MAX_ITEMS_PER_FEED   = 5
SUMMARY_SENTENCES    = 4
TRANSLATE_TITLES     = True
TRANSLATE_SUMMARIES  = True

# ðŸŒ TÃ¼m anahtar kelimeler (TR + EN)
GLOBAL_KEYWORDS = [
    # ðŸ§© Platformlar & Ekosistem
    "youtube","twitch","kick","tiktok","instagram","x.com","threads","rumble",
    "livestream","stream","streamer","creator","influencer","content creator",
    "broadcast","subscriber","followers","viewers","shorts","clip","ban","partner",
    "community","platform","streaming","upload","algorithm","monetization","feature",
    "viral","trend","controversy","backlash","criticism","tepki","tepki Ã§ekti","linÃ§",
    "drama","reaksiyon","yayÄ±n yasaÄŸÄ±","trend oldu","viral oldu",

    # ðŸŒ ÃœnlÃ¼ yayÄ±ncÄ±lar & internet figÃ¼rleri
    "mrbeast","ishowspeed","hasanabi","asmongold","xqc","kai cenat",
    "ludwig","ninja","pokimane","amouranth","valkyrae","shroud","drdisrespect",
    "ice poseidon","adin ross","nickmercs","summit1g","tfue","sykkuno",
    "myth","pewdiepie","dream","tommyinnit","markiplier","jacksepticeye",
    "logan paul","ksi","jake paul","moistcr1tikal","charli dâ€™amelio","bella poarch",

    # ðŸ’¬ Trendler & topluluk dinamikleri
    "reaction","drama","controversy","leak","clip","highlight","rage quit",
    "cancelled","apology","comeback","announcement","collab","partnership",
    "reveal","exclusive","interview","livestream fail","viral clip","top moment",
    "trending","memes","internet reaction","eleÅŸtirildi","skandal","tartÄ±ÅŸma",
    "gÃ¼ndem oldu","sosyal medya tepki","yayÄ±ncÄ± kavgasÄ±","clash","fued","debate",

    # ðŸ’° Creator economy & dijital iÅŸ dÃ¼nyasÄ±
    "sponsorship","deal","brand","agency","marketing","ads","revenue",
    "creator economy","influencer marketing","merch","startup","partnership",
    "brand deal","promotion","sponsorluk","iÅŸbirliÄŸi","ajans","kampanya","kazan",
    "income","platform change","exclusive deal","collaboration","network",
    "marka anlaÅŸmasÄ±","kampanya","tanÄ±tÄ±m videosu","sponsorlu iÃ§erik",

    # ðŸ‡¹ðŸ‡· TÃ¼rkÃ§e karÅŸÄ±lÄ±klar & yerel dijital kÃ¼ltÃ¼r
    "yayÄ±ncÄ±","influencer","iÃ§erik Ã¼retici","dijital kÃ¼ltÃ¼r","sosyal medya",
    "viral","akÄ±m","banlandÄ±","yasaklandÄ±","iÅŸbirliÄŸi","sponsor","ajans","anlaÅŸma",
    "abonelik","yayÄ±n kazancÄ±","platform deÄŸiÅŸikliÄŸi","trend oldu","komik video",
    "reaksiyon","twitch dramasÄ±","kick yayÄ±nÄ±","youtube videosu","sÄ±zdÄ±rÄ±ldÄ±",
    "takipÃ§i","izlenme","tÄ±klanma","algoritma","viral oldu","yayÄ±n yasaÄŸÄ±",
    "tepki Ã§ekti","tepki gÃ¶rdÃ¼","eleÅŸtirildi","gÃ¼ndem oldu","linÃ§ yedi"
]

# ðŸ”¤ Ã‡eviri sÄ±rasÄ±nda dokunulmamasÄ± gereken Ã¶zel isimler
PROPER_NOUNS = [
    "YouTube", "Twitch", "Kick", "Rumble",
    "MrBeast", "iShowSpeed", "HasanAbi", "Asmongold", "xQc",
    "Kai Cenat", "Ludwig", "Ninja", "Pokimane", "Amouranth",
    "Valkyrae", "Shroud", "Dr Disrespect", "Platcorn"
]

# ðŸŒ Kaynak isim eÅŸlemesi (gÃ¶rÃ¼nÃ¼r isimler)
PUBLISHER_MAP = {
    "www.dexerto.com": "Dexerto",
    "www.theverge.com": "The Verge",
    "www.ign.com": "IGN",
    "www.vulture.com": "Vulture",
    "www.hollywoodreporter.com": "Hollywood Reporter",
    "www.variety.com": "Variety",
    "www.gamespot.com": "GameSpot",
    "www.pcgamer.com": "PC Gamer",
    "www.kotaku.com": "Kotaku",
    "www.gamerbraves.com": "Gamer Braves",
    "www.hypebeast.com": "Hypebeast",
    "www.onedio.com": "Onedio",
    "www.sportskeeda.com": "Sportskeeda",
    "www.complex.com": "Complex",
    "www.gamingbible.com": "GamingBible",
    "www.reddit.com": "Reddit / LivestreamFail",
}

# ðŸ—žï¸ Tek kategori: Platcorn & Creator dÃ¼nyasÄ±
CATEGORIES = {
    "ðŸŸ¢ Platcorn & Creator": {
        "feeds": [
            # â€” Ä°ngilizce yayÄ±ncÄ±/creator haberleri â€”
            "https://www.dexerto.com/feed",
            "https://www.dexerto.com/streaming/feed",
            "https://www.dexerto.com/entertainment/feed",
            "https://www.dexerto.com/esports/feed",
            "https://www.theverge.com/creator-economy/rss/index.xml",
            "https://www.ign.com/rss",
            "https://www.kotaku.com/rss",
            "https://www.pcgamer.com/rss",
            "https://www.gamespot.com/feeds/news",
            "https://www.gamerbraves.com/feed/",
            "https://variety.com/feed/",
            "https://www.hollywoodreporter.com/feed/",
            "https://www.vulture.com/rss/all.xml",
            "https://screenrant.com/feed/",
            "https://www.hypebeast.com/feed",
            "https://www.tubefilter.com/feed/",
            "https://www.socialmediatoday.com/rss",
            "https://creatorhook.com/feed/",
            "https://passionfroot.me/blog/rss.xml",

            # â€” Reddit (sadece haber tarzÄ± iÃ§erik iÃ§in) â€”
            "https://www.reddit.com/r/LivestreamFail/new/.rss",

            # â€” TÃ¼rkÃ§e teknoloji ve eÄŸlence kaynaklarÄ± â€”
            "https://onedio.com/rss",
            "https://www.webtekno.com/rss",
            "https://shiftdelete.net/feed",
            "https://www.technopat.net/feed/",
            "https://www.log.com.tr/feed/",
            "https://www.donanimhaber.com/rss/tum/",

            # â€” RSS.app Ã¼zerinden eklenen Ã¶zel kaynaklar (X/IG & diÄŸerleri) â€”
            "https://rss.app/feeds/x6S2Zp6JUwCH1v0z.xml",
            "https://rss.app/feeds/U6QgNNMArHLnllsz.xml",
            "https://rss.app/feeds/ouzFl9q7fiqQ8kWC.xml",
            "https://rss.app/feeds/XxJ66s4xwq9qU2FW.xml",
            "https://rss.app/feeds/KutuqpKql1oBN51M.xml",
            "https://rss.app/feeds/uT33Zn9imAtSHeFb.xml",
            "https://rss.app/feeds/RQSjNahESu5puTnP.xml",
            "https://rss.app/feeds/we2ENM1QjscyHS6V.xml"  # Sportskeeda Streamers
        ],
        "keywords": GLOBAL_KEYWORDS
    }
}

APP_DIR = os.path.join(os.path.expanduser("~"), ".newsbot")
DB_PATH  = os.path.join(APP_DIR, "seen.db")

# =======================
# ARAÃ‡LAR
# =======================
def log(msg: str):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}", flush=True)

def ensure_app_dir():
    os.makedirs(APP_DIR, exist_ok=True)

def init_db():
    ensure_app_dir()
    conn = sqlite3.connect(DB_PATH)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS seen (
            id TEXT PRIMARY KEY,
            title TEXT,
            link TEXT,
            category TEXT,
            ts   INTEGER
        )
    """)
    conn.commit()
    return conn

def host_of(url):
    try:
        return urlparse(url).netloc
    except:
        return ""

def publisher_of(url):
    host = host_of(url).lower()
    return PUBLISHER_MAP.get(host, host)

def pretranslate_en(s: str) -> str:
    if not s: return s
    s = re.sub(r"\$?(\d+(\.\d+)?)\s?B\b", r"\1 billion", s, flags=re.IGNORECASE)
    s = re.sub(r"\$?(\d+(\.\d+)?)\s?M\b", r"\1 million", s, flags=re.IGNORECASE)
    s = re.sub(r"\$?(\d+(\.\d+)?)\s?K\b", r"\1 thousand", s, flags=re.IGNORECASE)
    s = s.replace("â€™","'").replace("â€œ","\"").replace("â€","\"")
    return s

def postprocess_money_tr(s: str) -> str:
    s = re.sub(r"\b([0-9]+(?:[.,][0-9]+)?)\s*million\b", r"\1 milyon", s, flags=re.IGNORECASE)
    s = re.sub(r"\b([0-9]+(?:[.,][0-9]+)?)\s*billion\b", r"\1 milyar", s, flags=re.IGNORECASE)
    s = re.sub(r"\b([0-9]+(?:[.,][0-9]+)?)\s*thousand\b", r"\1 bin", s, flags=re.IGNORECASE)
    s = re.sub(r"\$\s*([0-9])", r"$\1", s)
    return s

TITLE_VERB_MAP = [
    (r"\bleaks?\b", "ifÅŸa etti"),
    (r"\bclaims?\b", "iddia etti"),
    (r"\breveals?\b", "aÃ§Ä±kladÄ±"),
    (r"\bditching\b", "bÄ±rakmak"),
    (r"\bquits?\b", "bÄ±raktÄ±"),
    (r"\bwould make\b", "kazanacaÄŸÄ±nÄ±"),
    (r"\bslammed\b", "tepki Ã§ekti"),
    (r"\bshuts? down\b", "kapatÄ±ldÄ±"),
]

def polish_title_tr(tr: str) -> str:
    if not tr: return tr
    t = tr
    for pat, rep in TITLE_VERB_MAP:
        t = re.sub(pat, rep, t, flags=re.IGNORECASE)
    t = re.sub(r"\s+", " ", t).strip()
    if t and not t.endswith(('.', '!', '?')):
        t = t[0].upper() + t[1:]
    return t

def translate_en_to_tr(text: str, is_title=False) -> str:
    if not text: return text
    # Ã–zel isimleri yer tutucuya al
    placeholders = {}
    safe = text
    for i, name in enumerate(sorted(PROPER_NOUNS, key=len, reverse=True)):
        key = f"__PN{i}__"
        placeholders[key] = name
        safe = re.sub(rf"\b{name}\b", key, safe, flags=re.IGNORECASE)

    safe = pretranslate_en(safe)

    try:
        tr = GoogleTranslator(source="en", target="tr").translate(safe)
    except Exception:
        tr = safe

    for key, name in placeholders.items():
        tr = tr.replace(key, name)

    tr = postprocess_money_tr(tr)
    if is_title:
        tr = polish_title_tr(tr)
    return tr

def fetch_article_text(url: str) -> str:
    try:
        art = Article(url)
        art.download(); art.parse()
        return (art.text or "").strip()
    except Exception:
        return ""

def summarize_en(text: str, n_sent: int) -> str:
    text = re.sub(r"\s+", " ", (text or "").strip())
    if len(text.split()) < 60:
        return text
    try:
        parser = PlaintextParser.from_string(text, Tokenizer("english"))
        summarizer = LsaSummarizer()
        sents = summarizer(parser.document, n_sent)
        return " ".join(str(s) for s in sents)
    except Exception:
        return text

def bullets_tr(paragraph: str) -> str:
    """Uzun paragrafÄ± 3-5 maddeye bÃ¶lmeye Ã§alÄ±ÅŸ (TR Ã§Ä±ktÄ±yÄ± daha okunur yapar)."""
    if not paragraph: return paragraph
    sents = re.split(r"(?<=[.!?])\s+", paragraph)
    sents = [s.strip() for s in sents if s.strip()]
    if len(sents) <= 1:
        return paragraph
    sents = sents[:5]
    return "â€¢ " + "\nâ€¢ ".join(sents)

def tg_send(text: str):
    try:
        url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
        requests.post(url, json={
            "chat_id": TELEGRAM_CHAT_ID,
            "text": text,
            "parse_mode": "HTML",
            "disable_web_page_preview": False
        }, timeout=30).raise_for_status()
    except Exception as e:
        log(f"Telegram gÃ¶nderim hatasÄ±: {e}")

# --- TekrarlÄ± gÃ¶nderimi engellemek iÃ§in link normalize & stabil ID Ã¼retimi
TRACKING_PARAMS = {"utm_source","utm_medium","utm_campaign","utm_term","utm_content",
                   "si","fbclid","gclid","ref","igshid","mc_cid","mc_eid"}

def normalize_link(url: str) -> str:
    try:
        u = urlparse(url)
        q = [(k, v) for k, v in parse_qsl(u.query, keep_blank_values=True)
             if k.lower() not in TRACKING_PARAMS]
        clean = urlunparse((
            u.scheme or "https",
            (u.netloc or "").lower(),
            u.path.rstrip("/"),
            "", urlencode(q, doseq=True), ""   # temiz query
        ))
        return clean
    except Exception:
        return url or ""

def make_item_id(entry) -> str:
    """RSS Ã¶ÄŸesi iÃ§in stabil bir ID: id -> temiz link -> baÅŸlÄ±k; sonra SHA1."""
    eid   = getattr(entry, "id", "") or ""
    link  = normalize_link(getattr(entry, "link", "") or "")
    title = getattr(entry, "title", "") or ""
    base  = eid or link or title
    return hashlib.sha1(base.encode("utf-8", errors="ignore")).hexdigest()

def already_seen(conn, _id: str) -> bool:
    cur = conn.execute("SELECT 1 FROM seen WHERE id=?", (_id,))
    return cur.fetchone() is not None

def mark_seen(conn, _id: str, title: str, link: str, category: str):
    conn.execute(
        "INSERT OR IGNORE INTO seen (id,title,link,category,ts) VALUES (?,?,?,?, strftime('%s','now'))",
        (_id, title, link, category)
    )
    conn.commit()

def match_keywords(title: str, kw_list) -> bool:
    if not kw_list:
        return True
    t = (title or "").lower()
    return any(k.lower() in t for k in kw_list)

def build_feed_catalog():
    catalog = {}
    for cat, spec in CATEGORIES.items():
        for f in spec.get("feeds", []):
            catalog[f] = cat
    return catalog

# =======================
# ANA Ä°Åž
# =======================
def run_once():
    conn = init_db()
    catalog = build_feed_catalog()
    sent_total = 0
    run_seen_links = set()   # aynÄ± Ã§alÄ±ÅŸtÄ±rmada farklÄ± feedâ€™lerden gelen kopyalarÄ± engelle

    for feed_url, category in catalog.items():
        try:
            d = feedparser.parse(feed_url)
            entries = d.entries[:MAX_ITEMS_PER_FEED]
        except Exception as e:
            log(f"Feed hatasÄ±: {feed_url} -> {e}")
            continue

        cat_keywords = CATEGORIES.get(category, {}).get("keywords", [])

        for e in entries:
            _id   = make_item_id(e)
            link  = normalize_link(getattr(e, "link", "") or "")
            title = getattr(e, "title", "(baÅŸlÄ±ksÄ±z)")

            if already_seen(conn, _id):
                continue
            if link in run_seen_links:
                mark_seen(conn, _id, title, link, category)
                continue

            if not (match_keywords(title, GLOBAL_KEYWORDS) or match_keywords(title, cat_keywords)):
                mark_seen(conn, _id, title, link, category)
                continue

            # Makale metni -> Ã¶zet -> TR
            base_text  = fetch_article_text(link) or getattr(e, "summary", "") or getattr(e, "description", "")
            # RSS summary iÃ§indeki HTML etiketlerini temizle (Telegram uyumu)
            base_text  = re.sub(r'<[^>]+>', ' ', base_text or '')
            base_text  = re.sub(r'\s+', ' ', base_text).strip()

            summary_en = summarize_en(base_text, SUMMARY_SENTENCES)

            title_out   = translate_en_to_tr(title, is_title=True) if TRANSLATE_TITLES else title
            text_tr     = translate_en_to_tr(summary_en, is_title=False) if TRANSLATE_SUMMARIES else summary_en
            text_final  = bullets_tr(text_tr)

            pub = publisher_of(link)
            msg = f"{category}\n<b>{title_out}</b>\nKaynak: {pub} ({host_of(link)})\n\n{text_final}\n\nðŸ”— {link}"

            try:
                tg_send(msg)
                mark_seen(conn, _id, title, link, category)
                run_seen_links.add(link)
                sent_total += 1
                time.sleep(0.8)  # Telegram rate limit nazikliÄŸi
            except Exception as ex:
                log(f"GÃ¶nderim hatasÄ±: {title} -> {ex}")

    log(f"GÃ¶nderilen yeni Ã¶zet: {sent_total}")

def main():
    # GitHub Actions ortamÄ±nda tek tur Ã§alÄ±ÅŸ (cron tetiklemeleri iÃ§in)
    if os.getenv("GITHUB_ACTIONS", "").lower() == "true":
        run_once()
        return
    # Aksi halde yerel/PAâ€™de dÃ¶ngÃ¼
    if INTERVAL_SECONDS <= 0:
        run_once()
        return
    log(f"BaÅŸladÄ±. Her {INTERVAL_SECONDS} snâ€™de bir kontrol edilecek.")
    while True:
        run_once()
        time.sleep(INTERVAL_SECONDS)

if __name__ == "__main__":
    main()
